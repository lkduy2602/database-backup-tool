# =============================================================================
# DATABASE BACKUP TOOL - ENVIRONMENT VARIABLES CONFIGURATION
# =============================================================================
# 
# Copy this file to .env and modify the values according to your setup
# cp env.example .env
#
# =============================================================================

# =============================================================================
# QUICK START GUIDE
# =============================================================================
# 
# 1. Copy this file: cp env.example .env
# 2. Edit .env with your database details
# 3. Set up rclone (see RCLONE CONFIG SETUP below)
# 4. Run: docker-compose up --build
#
# =============================================================================

# =============================================================================
# DATABASE CONFIGURATION (REQUIRED)
# =============================================================================

# Type of database to backup
# Supported values: postgres, postgresql, mysql, mariadb, sqlite (mongodb temporarily removed)
DB_TYPE=postgres

# Database host address
# Examples: localhost, 127.0.0.1, db.example.com, 192.168.1.100
DB_HOST=localhost

# Database port number
# Examples: 5432 (PostgreSQL), 3306 (MySQL), 27017 (MongoDB)
DB_PORT=5432

# Database name to backup
# Examples: myapp, production_db, user_database
DB_NAME=your_database

# Database username for authentication
# Examples: postgres, root, admin, backup_user
DB_USER=your_username

# Database password for authentication
# Examples: secret123, mypassword, backup_password
DB_PASSWORD=your_password

# =============================================================================
# RCLONE CONFIGURATION (REQUIRED - CHOOSE ONE METHOD)
# =============================================================================
# Method 1: Mount config file (RECOMMENDED for complex configs)
RCLONE_CONFIG_FILE=/etc/rclone.conf
# Method 2: Base64 encoded config (GOOD for secrets management)
# RCLONE_CONFIG_BASE64="W2dkcml2ZV0KdHlwZSA9IGRyaXZlCnNjb3BlID0gZHJpdmUKc2VydmljZV9hY2NvdW50X2ZpbGUgPSAvcGF0aC90by9zZXJ2aWNlLWFjY291bnQuanNvbgp0ZWFtX2RyaXZlID0gCg=="
# Remote storage path in Google Drive
RCLONE_REMOTE_PATH=gdrive:backups/database

# =============================================================================
# RCLONE RATE LIMITING & OPTIMIZATION (OPTIONAL)
# =============================================================================
# These settings help avoid Google Drive API quota exceeded errors
# All settings have very conservative defaults and are optional

# Transactions per second limit for rclone operations
# Default: 1 (ultra-conservative limit based on rclone forum recommendations)
# Lower values = slower but safer, Higher values = faster but risk quota limits
# Examples: 1, 2, 3 (recommended: 1 for large files, 1-2 for small files)
# WARNING: Higher values may cause "Quota exceeded for quota metric 'Queries'" errors
# Based on: https://forum.rclone.org/t/best-practices-for-handling-aggressive-google-drive-rate-limiting/40618
RCLONE_TPS_LIMIT=1

# Chunk size for large file uploads (in bytes with suffix: K, M, G)
# Default: 16M (ultra-conservative balance based on rclone forum recommendations)
# Larger chunks = fewer API calls but more memory usage
# Examples: 8M, 16M, 32M, 64M (recommended: 8M-32M)
# WARNING: Larger chunks may cause quota exceeded errors with large files
# Based on: https://forum.rclone.org/t/best-practices-for-handling-aggressive-google-drive-rate-limiting/40618
RCLONE_CHUNK_SIZE=16M

# Upload cutoff threshold for chunked uploads (in bytes with suffix: K, M, G)
# Default: 16M (forces chunked upload for files larger than this)
# Should match or be smaller than RCLONE_CHUNK_SIZE
# Examples: 8M, 16M, 32M (recommended: same as RCLONE_CHUNK_SIZE)
RCLONE_UPLOAD_CUTOFF=16M

# Number of concurrent transfers
# Default: 1 (single transfer for backup operations)
# Higher values may cause quota limits with large files
# Examples: 1, 2, 4 (recommended: 1 for backups)
RCLONE_TRANSFERS=1

# Number of checkers for file operations
# Default: 2 (very conservative based on rclone forum recommendations)
# Higher values may cause rate limits
# Examples: 1, 2, 4 (recommended: 1-2 for large files)
# Based on: https://forum.rclone.org/t/best-practices-for-handling-aggressive-google-drive-rate-limiting/40618
RCLONE_CHECKERS=2

# Daily transfer limit to avoid quota issues
# Default: 700G (conservative limit below Google's 750GB per 24h limit)
# Examples: 500G, 700G, 750G (recommended: stay below 750G)
# WARNING: Exceeding 750GB per 24h will cause quota exceeded errors
RCLONE_MAX_TRANSFER=700G

# =============================================================================
# BACKUP NAMING CONFIGURATION (OPTIONAL)
# =============================================================================

# Option 1: Custom filename template (HIGHEST PRIORITY)
# If set, this will completely override the default naming scheme
# Available placeholders:
#   {db_type}     - Database type (postgres, mysql, mongodb, sqlite)
#   {db_name}     - Database name
#   {timestamp}   - Full timestamp (YYYYMMDD_HHMMSS)
#   {date}        - Date only (YYYYMMDD)
#   {time}        - Time only (HHMMSS)
#   {host}        - Database host
#   {port}        - Database port
#
# Examples:
# BACKUP_NAME_TEMPLATE={db_name}_{db_type}_{date}_{time}.backup
# BACKUP_NAME_TEMPLATE=myapp_{db_type}_{timestamp}
# BACKUP_NAME_TEMPLATE=production_{host}_{db_name}_{date}
# BACKUP_NAME_TEMPLATE={db_type}_backup_{timestamp}.sql
# BACKUP_NAME_TEMPLATE=daily_{db_name}_{date}.archive
# BACKUP_NAME_TEMPLATE={host}_{db_type}_{timestamp}.backup
# BACKUP_NAME_TEMPLATE={db_name}_{date}_{time}.{db_type}

# Option 2: Simple prefix (used only if template is not set)
# Examples: myapp, production, staging, dev, backup
# Default: "backup" if not set
BACKUP_NAME_PREFIX=myapp

# =============================================================================
# BACKUP AUTOMATION CONFIGURATION (OPTIONAL)
# =============================================================================

# Enable automated backup mode with cron
# Values: true, false, 1, 0
# true/1 = automated mode with cron jobs
# false/0 = manual mode (run once and exit)
AUTOMATED_BACKUP=false

# Cron schedule for automated backups (only used when AUTOMATED_BACKUP=true)
# Format: minute hour day month weekday
# Examples:
#   "0 2 * * *"     - Daily at 2:00 AM (default)
#   "0 2,14 * * *"  - Twice daily at 2:00 AM and 2:00 PM
#   "0 */6 * * *"   - Every 6 hours
#   "0 2 * * 0"     - Weekly on Sunday at 2:00 AM
#   "0 0 1 * *"     - Monthly on 1st at midnight
#   "0 2 * * 1-5"   - Weekdays at 2:00 AM
#   "*/30 * * * *"  - Every 30 minutes
CRON_SCHEDULE=0 2 * * *

# =============================================================================
# OPTIONAL CONFIGURATION
# =============================================================================

# Backup retention period in days
# Old backups older than this number of days will be automatically deleted
# Examples: 7, 30, 90, 365
# Set to empty or comment out to disable retention policy
BACKUP_RETENTION_DAYS=30

# Timezone for backup timestamps and scheduling
# Examples: Asia/Ho_Chi_Minh, America/New_York, Europe/London, UTC
# Default: system timezone if not set
TZ=Asia/Ho_Chi_Minh

# =============================================================================
# USAGE EXAMPLES
# =============================================================================

# Example 1: Simple PostgreSQL backup with file mount
# DB_TYPE=postgres
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=myapp
# DB_USER=postgres
# DB_PASSWORD=mypassword
# RCLONE_CONFIG_FILE=/etc/rclone.conf
# RCLONE_REMOTE_PATH=gdrive:backups
# BACKUP_NAME_PREFIX=myapp
# RCLONE_TPS_LIMIT=1
# RCLONE_CHUNK_SIZE=16M
# RCLONE_UPLOAD_CUTOFF=16M
# RCLONE_CHECKERS=2
# RCLONE_MAX_TRANSFER=700G

# Example 2: Production MySQL with base64 config and rate limiting
# DB_TYPE=mysql
# DB_HOST=prod-db.example.com
# DB_PORT=3306
# DB_NAME=production
# DB_USER=backup_user
# DB_PASSWORD=secure_password
# RCLONE_CONFIG_BASE64=W2dkcml2ZV0KdHlwZSA9IGRyaXZlCnNjb3BlID0gZHJpdmUKc2VydmljZV9hY2NvdW50X2ZpbGUgPSAvcGF0aC90by9zZXJ2aWNlLWFjY291bnQuanNvbgp0ZWFtX2RyaXZlID0gCg==
# RCLONE_REMOTE_PATH=gdrive:production/backups
# BACKUP_NAME_TEMPLATE=prod_{host}_{db_name}_{date}_{time}.sql
# AUTOMATED_BACKUP=true
# CRON_SCHEDULE=0 2,14 * * *
# BACKUP_RETENTION_DAYS=90
# RCLONE_TPS_LIMIT=2
# RCLONE_CHUNK_SIZE=32M
# RCLONE_UPLOAD_CUTOFF=32M
# RCLONE_TRANSFERS=1

# Example 3: Large Database with Conservative Rate Limiting
# For very large databases (>1GB) that frequently hit rate limits
# DB_TYPE=postgres
# DB_HOST=large-db.example.com
# DB_PORT=5432
# DB_NAME=large_database
# DB_USER=backup_user
# DB_PASSWORD=secure_password
# RCLONE_CONFIG_FILE=/etc/rclone.conf
# RCLONE_REMOTE_PATH=gdrive:large_backups
# BACKUP_NAME_TEMPLATE=large_{db_name}_{date}_{time}.backup
# AUTOMATED_BACKUP=true
# CRON_SCHEDULE=0 1 * * *
# BACKUP_RETENTION_DAYS=30
# RCLONE_TPS_LIMIT=1
# RCLONE_CHUNK_SIZE=16M
# RCLONE_UPLOAD_CUTOFF=16M
# RCLONE_TRANSFERS=1

# Example 4: Small Database with Moderate Settings
# For small databases (<100MB) where speed is more important
# DB_TYPE=sqlite
# DB_HOST=localhost
# DB_PORT=0
# DB_NAME=small_app.db
# DB_USER=
# DB_PASSWORD=
# RCLONE_CONFIG_FILE=/etc/rclone.conf
# RCLONE_REMOTE_PATH=gdrive:small_backups
# BACKUP_NAME_PREFIX=small
# AUTOMATED_BACKUP=true
# CRON_SCHEDULE=0 */4 * * *
# RCLONE_TPS_LIMIT=3
# RCLONE_CHUNK_SIZE=64M
# RCLONE_UPLOAD_CUTOFF=64M

# Example 5: MongoDB temporarily removed
# Will be added back when MongoDB tools are properly configured

# =============================================================================
# RCLONE CONFIG SETUP INSTRUCTIONS
# =============================================================================

# Step 1: Create rclone config locally
# rclone config create gdrive drive scope drive --non-interactive

# Step 2: Choose your method:

# Method 1 - File Mount (Recommended):
# 1. Copy your rclone.conf to project directory
# 2. In docker-compose.yml, add volume mount:
#    volumes:
#      - ./rclone.conf:/etc/rclone.conf:ro
# 3. Set RCLONE_CONFIG_FILE=/etc/rclone.conf

# Method 2 - Base64 (Good for secrets):
# 1. Encode your config: cat rclone.conf | base64 -w 0
# 2. Copy the output and set as RCLONE_CONFIG_BASE64

# =============================================================================
# SECURITY NOTES
# =============================================================================
# 
# 1. Never commit .env file to version control
# 2. Use strong, unique passwords for database users
# 3. Limit database user permissions to backup operations only
# 4. Use service accounts for Google Drive (not personal accounts)
# 5. Rotate credentials regularly
# 6. Monitor backup logs for any unauthorized access attempts
# 7. For production, prefer Method 1 (file mount) or Method 2 (base64)
# 8. Use Method 1 (file mount) for complex configs with special characters
# 9. Consider using Docker secrets for production deployments
# 10. Regularly audit backup access logs
#
# =============================================================================

# =============================================================================
# PERFORMANCE TIPS & QUOTA MANAGEMENT
# =============================================================================
# 
# 1. Use streaming backup (default) to avoid disk space issues
# 2. Schedule backups during low-traffic hours
# 3. Monitor backup duration and adjust CRON_SCHEDULE accordingly
# 4. Use appropriate BACKUP_RETENTION_DAYS to balance storage and safety
# 5. Consider network bandwidth when choosing backup frequency
# 6. Test backup restore process regularly
# 7. For very large databases (>1GB), use ultra-conservative settings:
#    - RCLONE_TPS_LIMIT=1
#    - RCLONE_CHUNK_SIZE=16M
#    - RCLONE_UPLOAD_CUTOFF=16M
# 8. For large databases (500MB-1GB), use conservative settings:
#    - RCLONE_TPS_LIMIT=1-2
#    - RCLONE_CHUNK_SIZE=32M
#    - RCLONE_UPLOAD_CUTOFF=32M
# 9. For medium databases (100MB-500MB), use moderate settings:
#    - RCLONE_TPS_LIMIT=2
#    - RCLONE_CHUNK_SIZE=32M-64M
#    - RCLONE_UPLOAD_CUTOFF=32M-64M
# 10. For small databases (<100MB), you can use more aggressive settings:
#     - RCLONE_TPS_LIMIT=2-3
#     - RCLONE_CHUNK_SIZE=64M-128M
#     - RCLONE_UPLOAD_CUTOFF=64M-128M
# 11. Monitor Google Drive API quota usage in Google Cloud Console
# 12. If you hit quota limits frequently, reduce TPS_LIMIT and CHUNK_SIZE
# 13. The tool now includes advanced retry logic (5 retries with 30s delay)
# 14. Additional optimizations: drive-acknowledge-abuse, drive-keep-revision-forever, drive-skip-gdocs
# 15. Use the quota checker script: docker-compose exec db-backup /app/check-quota.sh
#
# =============================================================================

# =============================================================================
# TROUBLESHOOTING QUOTA ISSUES
# =============================================================================
#
# If you encounter "Quota exceeded for quota metric 'Queries'" errors:
#
# 1. IMMEDIATE ACTIONS:
#    - Reduce RCLONE_TPS_LIMIT to 1
#    - Reduce RCLONE_CHUNK_SIZE to 16M
#    - Wait 1-2 minutes before retrying
#
# 2. LONG-TERM SOLUTIONS:
#    - Use different Google account/project
#    - Request quota increase from Google Cloud Console
#    - Schedule backups during off-peak hours
#    - Reduce backup frequency
#
# 3. MONITORING:
#    - Check Google Cloud Console > APIs & Services > Quotas
#    - Monitor backup logs for rate limit patterns
#    - Use the built-in quota checker script
#
# =============================================================================
